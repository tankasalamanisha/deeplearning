{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399d9b68",
   "metadata": {},
   "source": [
    "## A comparitive notebook for implimenting Logistic Regression: Traditional ML vs Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a2682",
   "metadata": {},
   "source": [
    "### Vectorizing Logistic Regression\n",
    "\n",
    "In this notebook, we shall first delve into what happens in logistic regression algorithm and understand if parts of it can be vectorized. Once we understand it, we get into learning how to vectorize gradient descent output in Logistic Regression. The step further would be our comparitive implimentation of Logistic regression using traditional ML and Deep Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00656334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non vectorised cost: [[0.69311513]]\n",
      "Time taken (non vectorised) : 0.04388141632080078 seconds\n"
     ]
    }
   ],
   "source": [
    "# Non vectorised approach\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "# Initialization\n",
    "m = 1000 # number of examples\n",
    "n= 5 # number of features\n",
    "W = np.random.randn(n,1) * 0.01 # weights\n",
    "b = 0 # bias\n",
    "X = np.random.randn(n,m) # input data\n",
    "Y = np.random.randint(0,2,size=(1,m)) # labels\n",
    "alpha = 0.01 # learning rate\n",
    "\n",
    "# Forward pass and backward pass initialization\n",
    "dW = np.zeros((n,1))\n",
    "db = 0\n",
    "cost = 0\n",
    "\n",
    "# Forward pass:\n",
    "\n",
    "for i in range(m):\n",
    "    z_i = np.dot(W.T, X[:,i].reshape(-1,1)) + b # linear step\n",
    "    a_i = 1/(1+np.exp(-z_i)) # activation step (sigmoid = logistic)\n",
    "    cost  += - (Y[0,i] * np.log(a_i) + (1 - Y[0,i]) * np.log(1-a_i)) # cost computation\n",
    "\n",
    "    # Backward pass:\n",
    "    dz_i = a_i - Y[0,i] # derivative of cost wrt z\n",
    "    dW += X[:,i].reshape(-1,1) * dz_i # gradient at the step\n",
    "    db += dz_i\n",
    "\n",
    "# Average cost and gradients\n",
    "cost /= m\n",
    "dW /= m\n",
    "db /= m\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"Non vectorised cost: {cost}\")\n",
    "print(f\"Time taken (non vectorised) : {toc - tic} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8830c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorised cost: 0.6931151349307489\n",
      "Time taken (vectorised) : 0.003748655319213867 seconds\n"
     ]
    }
   ],
   "source": [
    "# Vectorised approach\n",
    "\n",
    "tic = time.time()\n",
    "Z = np.dot(W.T, X) + b # linear step\n",
    "A = 1/(1+np.exp(-Z)) # activation step (sigmoid = logistic)\n",
    "cost = - (Y * np.log(A) + (1 - Y) * np.log(1-A)) # cost computation\n",
    "cost = np.sum(cost) / m # average cost\n",
    "\n",
    "# Backward pass:\n",
    "dZ = A - Y # derivative of cost wrt z\n",
    "dW = np.dot(X, dZ.T) / m # gradient at the step\n",
    "db = np.sum(dZ) / m # gradient at the step\n",
    "\n",
    "toc = time.time()\n",
    "print(f\"Vectorised cost: {cost}\")\n",
    "print(f\"Time taken (vectorised) : {toc - tic} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be1ef6",
   "metadata": {},
   "source": [
    "## Next step: Logistic Regression through the lens of Deep Learning\n",
    "\n",
    "In this step we will start implimenting modularised functions for understanding how logistic regression works through the lens of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871446d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "def initialize_weights(n):\n",
    "    W = np.random.randn(n,1) * 0.01\n",
    "    b = 0\n",
    "    return W, b\n",
    "def compute_cost(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = - (Y * np.log(A) + (1 - Y) * np.log(1-A))\n",
    "    cost = np.sum(cost) / m\n",
    "    return cost\n",
    "def compute_gradients(X, A, Y):\n",
    "    m = Y.shape[1]\n",
    "    dZ = A - Y\n",
    "    dW = np.dot(X, dZ.T) / m\n",
    "    db = np.sum(dZ) / m\n",
    "    return dW, db\n",
    "def optimize(W, b, X, Y, alpha, num_iterations):\n",
    "    for i in range(num_iterations):\n",
    "        Z = np.dot(W.T, X) + b\n",
    "        A = sigmoid(Z)\n",
    "        cost = compute_cost(A, Y)\n",
    "        dW, db = compute_gradients(X, A, Y)\n",
    "        W -= alpha * dW\n",
    "        b -= alpha * db\n",
    "    return W, b\n",
    "def predict(W, b, X):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    predictions = (A > 0.5).astype(int)\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
